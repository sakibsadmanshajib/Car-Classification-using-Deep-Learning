{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision: Classifying car make, model and year\n",
    "\n",
    "Computer vision could potentially be used to automate traffic censuses and other tasks that require identification of vehicles.\n",
    "The <a href=\"https://www.tensorflow.org/datasets/catalog/cars196\">cars196</a> dataset contains 16,185 images of 196 different types of cars, which\n",
    "can be used to train a supervised learning system to determine the make and model of a vehicle in a photograph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:01.062823Z",
     "iopub.status.busy": "2021-09-11T22:58:01.062451Z",
     "iopub.status.idle": "2021-09-11T22:58:05.851878Z",
     "shell.execute_reply": "2021-09-11T22:58:05.851134Z",
     "shell.execute_reply.started": "2021-09-11T22:58:01.06279Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:05.85395Z",
     "iopub.status.busy": "2021-09-11T22:58:05.853639Z",
     "iopub.status.idle": "2021-09-11T22:58:07.813008Z",
     "shell.execute_reply": "2021-09-11T22:58:07.812095Z",
     "shell.execute_reply.started": "2021-09-11T22:58:05.853915Z"
    }
   },
   "outputs": [],
   "source": [
    "####NOTE THIS IS CODE THAT MUST BE RUN ON KAGGL.\n",
    "#If NON KAGGLE environment then CHANGE DATA_DIR to the location of DATASET and download = true\n",
    "#best working in Kaggle\n",
    "\n",
    "DATA_DIR = '/kaggle/input/cars196'\n",
    "\n",
    "[train_ds, test_ds], ds_info = tfds.load(\n",
    "    \"cars196\",\n",
    "    # Reserve 10% for validation and 50% for test\n",
    "    #loads dataset into tf.load.dataset (Luckily TFDS)\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,  # Include labels\n",
    "    with_info=True,\n",
    "    download=False,\n",
    "    data_dir=DATA_DIR,  #Kaggle input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the built-in visualization function to show some example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:07.814689Z",
     "iopub.status.busy": "2021-09-11T22:58:07.814347Z",
     "iopub.status.idle": "2021-09-11T22:58:09.831337Z",
     "shell.execute_reply": "2021-09-11T22:58:09.830313Z",
     "shell.execute_reply.started": "2021-09-11T22:58:07.814653Z"
    }
   },
   "outputs": [],
   "source": [
    "tfds.visualization.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-12T21:41:32.153521Z",
     "iopub.status.busy": "2021-09-12T21:41:32.152898Z",
     "iopub.status.idle": "2021-09-12T21:41:32.186327Z",
     "shell.execute_reply": "2021-09-12T21:41:32.184910Z",
     "shell.execute_reply.started": "2021-09-12T21:41:32.153478Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8def07a51ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the data\n",
    "Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn't a great fit for feeding a neural network. We need to do 2 things:\n",
    "\n",
    "* Standardize to a fixed image size. We pick 150x150.\n",
    "* Normalize pixel values between -1 and 1. We'll do this using a Normalization layer as part of the model itself.\n",
    "\n",
    "In general, it's a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you'll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\n",
    "\n",
    "Here, we'll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we'll do the input value scaling as part of the model, when we create it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:09.843233Z",
     "iopub.status.busy": "2021-09-11T22:58:09.842882Z",
     "iopub.status.idle": "2021-09-11T22:58:09.851334Z",
     "shell.execute_reply": "2021-09-11T22:58:09.850315Z",
     "shell.execute_reply.started": "2021-09-11T22:58:09.843197Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:09.853492Z",
     "iopub.status.busy": "2021-09-11T22:58:09.85311Z",
     "iopub.status.idle": "2021-09-11T22:58:09.922904Z",
     "shell.execute_reply": "2021-09-11T22:58:09.922081Z",
     "shell.execute_reply.started": "2021-09-11T22:58:09.853454Z"
    }
   },
   "outputs": [],
   "source": [
    "#resize images to 150x150:\n",
    "height, width = 150, 150\n",
    "#size = (150, 150)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, (height, width)), y))\n",
    "#validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, (height, width)), y))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Resizing and random data augmentation\n",
    "\n",
    "When you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n",
    "\n",
    "Additionally, let's the data and use caching and prefetching to optimize load speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:09.930886Z",
     "iopub.status.busy": "2021-09-11T22:58:09.930281Z",
     "iopub.status.idle": "2021-09-11T22:58:10.102759Z",
     "shell.execute_reply": "2021-09-11T22:58:10.10207Z",
     "shell.execute_reply.started": "2021-09-11T22:58:09.930848Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # sdantard batch size for images\n",
    "\n",
    "def augment_func(image,label):\n",
    "    image = tf.image.resize_with_crop_or_pad(image,height+6,width+6)\n",
    "    #image = tf.clip_by_value(image,0,255) #make sure you have no color value higher than 225 or lower than 0.\n",
    "    image = tf.image.random_crop(image,size=[height,width,3])\n",
    "    image = tf.image.random_flip_left_right(image) #different aspect of vehicles\n",
    "    image = tf.image.random_hue(image,0.2) #random color, change a red cat into a blue car\n",
    "    image = tf.image.random_contrast(image,0.5,2)# random contrast\n",
    "    image = tf.image.random_saturation(image,0,2)# random sturations\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_ds = train_ds.cache().map(augment_func).shuffle(100).batch(batch_size).prefetch(buffer_size=10) # cache makes the images ready before running\n",
    "test_ds = test_ds.cache().map(augment_func).batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what the first 18 images of the first batch looks like after various random transformations.\n",
    "\n",
    "Note that because the augmentations in the previous cell are applied randomly, these images will look different everytime they are run through the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:10.104756Z",
     "iopub.status.busy": "2021-09-11T22:58:10.104391Z",
     "iopub.status.idle": "2021-09-11T22:58:13.602981Z",
     "shell.execute_reply": "2021-09-11T22:58:13.602203Z",
     "shell.execute_reply.started": "2021-09-11T22:58:10.10472Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "for i, (image_batch, label) in enumerate(train_ds.take(18)): # we did batch(batch_size) before, if we didn't, \"take\" will take individual image.\n",
    "        ax = plt.subplot(6, 3, i + 1)\n",
    "        plt.imshow(image_batch[3].numpy().astype(\"int32\")) #tensor flow treats things as floating numbers, but images need integer.\n",
    "        plt.title(ds_info.features[\"label\"].names[int(label[3])])\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:13.604678Z",
     "iopub.status.busy": "2021-09-11T22:58:13.604189Z",
     "iopub.status.idle": "2021-09-11T22:58:13.61035Z",
     "shell.execute_reply": "2021-09-11T22:58:13.609618Z",
     "shell.execute_reply.started": "2021-09-11T22:58:13.604638Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "\n",
    "Now let's built a model.\n",
    "\n",
    "1. We add a Normalization layer to scale input values (initially in the [0, 255] range) to the [-1, 1] range, because this is the format that is expected by the pre-trained model that comes next.\n",
    "1. We start with a pre-trained model that's trained on the [ImageNet](http://image-net.org/about-overview) dataset, which includes a large number of images with a large number of different labels, but doesn't not include as much specificity regarding vehicle types as the cars196 dataset does. Training these models from scratch is tricky; it is much easier to start with a pre-trained model and fine tune it for use for a different task.\n",
    "3. We add our own classification layer at the end of the model, with 96 outputs representing our 96 vehicle classes, and \"softmax\" activation which forces the output values to all be between 0 and 1, and to all sum to 1.\n",
    "4. We add a Dropout layer before the above classification layer, for regularization.\n",
    "\n",
    "\n",
    "We need the number of outputs in the final layer to equal the number of variables or classes we want to predict: in this case, 196 vehicle types. \n",
    "We use a softmax activation on the final layer for classification problems, but if we want to use this model for regression we would only have to change the number of desired outputs and set `activation=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:13.612203Z",
     "iopub.status.busy": "2021-09-11T22:58:13.611643Z",
     "iopub.status.idle": "2021-09-11T22:58:16.344215Z",
     "shell.execute_reply": "2021-09-11T22:58:16.343471Z",
     "shell.execute_reply.started": "2021-09-11T22:58:13.612157Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(height, width, 3),\n",
    "    include_top=False, # Do not include the final ImageNet classifier layer at the top.\n",
    ")  \n",
    "\n",
    "base_model.trainable = False # We freeze the base model\n",
    "\n",
    "# Create new model on surrounding our pretrained base model.\n",
    "inputs = tf.keras.Input(shape=(height, width, 3))\n",
    "\n",
    "# Pre-trained Xception weights requires that input be normalized\n",
    "# from (0, 255) to a range (-1., +1.), the normalization layer\n",
    "# does the following, outputs = (inputs - mean) / sqrt(var)\n",
    "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
    "mean = np.array([127.5] * 3)\n",
    "var = mean ** 2\n",
    "# Scale inputs to [-1, +1]\n",
    "x = norm_layer(inputs)\n",
    "norm_layer.set_weights([mean, var])\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "# during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\n",
    "#  normalize its inputs during inference after having been trained on data that has similar statistics as the inference data.\n",
    "#The layer will transform inputs so that they are standardized, meaning that they \n",
    "# will have a mean of zero and a standard deviation of one. During training, \n",
    "# the layer will keep track of statistics for each input variable and use them to standardize the data.\n",
    "# This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x) # this is a neural network operation to help adapt the features learned by the pretrained model to our specific task. \n",
    "                                             #he 2D Global average pooling block takes a tensor of size (input width) x (input height) x (input channels) \n",
    "                                            #and computes the average value of all values across the entire (input width) x (input height) matrix for each of the (input channels).\n",
    "                                            #designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer.\n",
    "x = keras.layers.Dropout(0.5)(x)  # Regularize with dropout\n",
    "num_outputs = ds_info.features['label'].num_classes # This is the number of output variables we want, 196 in this case.\n",
    "outputs = keras.layers.Dense(num_outputs, activation=\"softmax\")(x) # Use activation=softmax for classification, and activation=None for regression.\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:58:16.347323Z",
     "iopub.status.busy": "2021-09-11T22:58:16.347049Z",
     "iopub.status.idle": "2021-09-11T23:35:11.75809Z",
     "shell.execute_reply": "2021-09-11T23:35:11.757355Z",
     "shell.execute_reply.started": "2021-09-11T22:58:16.347294Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "model.fit(train_ds, epochs=epochs,validation_data = test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "We use a relatively low learning rate to prevent the model from unlearning what it learned when being trained on the larger imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T23:35:11.761332Z",
     "iopub.status.busy": "2021-09-11T23:35:11.76106Z",
     "iopub.status.idle": "2021-09-12T01:16:59.610201Z",
     "shell.execute_reply": "2021-09-12T01:16:59.609335Z",
     "shell.execute_reply.started": "2021-09-11T23:35:11.761303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "epochs = 100\n",
    "model.fit(train_ds, epochs = epochs, validation_data = test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the model for later use. If you are doing this on Kaggle, there is an option to download the saved file in the panel on the right side of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-12T01:16:59.611994Z",
     "iopub.status.busy": "2021-09-12T01:16:59.611639Z",
     "iopub.status.idle": "2021-09-12T01:17:00.399734Z",
     "shell.execute_reply": "2021-09-12T01:17:00.398917Z",
     "shell.execute_reply.started": "2021-09-12T01:16:59.611956Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"model.h5\", save_format=\"h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
